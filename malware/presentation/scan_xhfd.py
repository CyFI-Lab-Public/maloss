import signal
import logging
from multiprocessing import Pool


def crawl(url, anchor="xhfd", result_file="js-metrics.sites.txt", detail_file="js-metrics.detail.txt", disable_js=False):
    driver = None
    # xhfda, X9S6XQFx, 2e2f746573742f64617461, ___cfduid, XLeVP82h, "(c)(module.exports, require, req, res, next)", 63R10B, 1111nardski@gmail.com
    try:
        logging.warning("crawling %s", url)
        if disable_js:
            import requests
            url_content = requests.request('GET', url)
            if url_content.text and anchor in url_content.text:
                logging.warning("%s contains text %s", url, anchor)
                open(result_file, 'a').write(url + '\n')
                open(detail_file, 'a').write('url: %s\n%s\n\n\n\n' % (url, url_content.text))
        else:
            # crawl website with js support
            # https://stackoverflow.com/questions/8049520/web-scraping-javascript-page-with-python
            # install phantomjs as well
            # https://phantomjs.org/download.html
            from selenium import webdriver
            driver = webdriver.PhantomJS()
            driver.get(url)
            page_source = driver.page_source
            if page_source and anchor in page_source:
                logging.warning("%s contains text %s", url, anchor)
                open(result_file, 'a').write(url + '\n')
                open(detail_file, 'a').write('url: %s\n%s\n\n\n\n' % (url, page_source))
            # https://adiyatmubarak.wordpress.com/2017/03/29/python-fix-oserror-errno-9-bad-file-descriptor-in-selenium-using-phantomjs/
            driver.service.process.send_signal(signal.SIGTERM)
            driver.quit() 
    except Exception as e:
        logging.error("error crawling %s: %s, ignoring!", url, str(e))
        if driver:
            driver.quit()


def main():
    top_1m = 'top-1m.csv'
    domains = [line.strip().split(',')[-1] for line in open(top_1m, 'r')]
    urls = ["https://%s" % domain for domain in domains]
    logging.warning("there are %d urls", len(urls))
    pool = Pool(processes=20)
    pool.map(crawl, urls)


if __name__ == "__main__":
    main()

